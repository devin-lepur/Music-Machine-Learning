{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJY22X9rfTko"
   },
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXMj6OAIWaIE",
    "outputId": "f8495e1a-dee9-4ee9-8f68-02498c0d5ad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Knk8UsneWgVV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4cZZu8IWy4S"
   },
   "outputs": [],
   "source": [
    "#Read datasets and add 'isLiked' column with default values to both dataframes\n",
    "liked = pd.read_csv(\"/content/drive/MyDrive/MusicMachineLearningProject/database_likes.csv\")\n",
    "disliked = pd.read_csv(\"/content/drive/MyDrive/MusicMachineLearningProject/database_dislikes.csv\")\n",
    "liked[\"isLiked\"] = True\n",
    "disliked[\"isLiked\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwNy5UhHXJh9"
   },
   "outputs": [],
   "source": [
    "#Drop any rows with missing values (ie. local files)\n",
    "liked.dropna(inplace=True)\n",
    "disliked.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuGa_aWxZNKb"
   },
   "outputs": [],
   "source": [
    "#Merge the dataframes, drop duplicate songs\n",
    "merged = pd.concat([liked,disliked])\n",
    "merged.drop_duplicates(subset=[\"Spotify ID\"], inplace=True)\n",
    "merged.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2S2iDNu_afSi",
    "outputId": "f8a19e89-a9a6-4c33-e1de-4c99905b87ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27, 31],\n",
       "       [ 8, 87]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split Data\n",
    "X = merged.drop(['Spotify ID', 'Artist IDs', 'Track Name', 'Album Name',\n",
    "       'Artist Name(s)', 'Release Date', 'Duration (ms)', 'Instrumentalness', 'Added By', 'Added At', 'Genres', 'isLiked'], axis=1)\n",
    "y = merged['isLiked']\n",
    "\n",
    "#Create scaler for normalization\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "#Train model using logistic regression and test size of 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "model = LogisticRegression(random_state=101, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Create a confusion matrix\n",
    "predictions = model.predict(X_test)\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ep7AZZLkQNC",
    "outputId": "091a0e1a-746d-4fcf-9586-b68f9ce77d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Disliked       0.77      0.47      0.58        58\n",
      "       Liked       0.74      0.92      0.82        95\n",
      "\n",
      "    accuracy                           0.75       153\n",
      "   macro avg       0.75      0.69      0.70       153\n",
      "weighted avg       0.75      0.75      0.73       153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display report\n",
    "target_names = ['Disliked', 'Liked']\n",
    "print(metrics.classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Soc24jF9aWvn"
   },
   "source": [
    "The model has an accuracy of 75%. That is, 75% of all songs evaluated were evaluated correctly. This is higher than I really imagined. I was expecting an accuracy of 40%-60%, close to that of a coin flip, but the model seems to have exceeded that. Soon I hope to evaluate this model against two more datasets. First, a cumulative playlist of all my friends (Rap being likely the most popular here along side branches into several other, distinct genres). The goal here will be to simply see how good the model is a picking out new songs and see if it is viable to be a time saver. Second, is to run it against a playlist of my sister, moslty just for fun and curiosity. The model has very little data from the genres she listens to so I am curious both what it would do with this data that largely doesn't conform to the \"liked\" or \"disliked\" data as well as see if it is still able to pick out songs that I would like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvVGf7ySenZp"
   },
   "source": [
    "Additionally I hope to improve this number potentially using the artist names or seeking out another model type. I believe that 80% accuracy would be impressive for the nature of the data as simply being much higher than I could expect."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
